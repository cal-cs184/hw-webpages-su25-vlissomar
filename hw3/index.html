<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body>
		<div class="container">
		<h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Names: Sophia Liu</div>

		<br>

		Link to webpage: <a href="https://cal-cs184.github.io/hw-webpages-su25-vlissomar/hw3/index.html">https://cal-cs184.github.io/hw-webpages-su25-vlissomar/hw3/index.html</a>
		<br>
		Link to GitHub repository: <a href="https://github.com/cal-cs184/hw-pathtracer-updated-orz4249">cs184.eecs.berkeley.edu/sp25</a>
		
		<figure>
			<img src="cornell.png" alt="Cornell Boxes with Bunnies" style="width:70%"/>
			<figcaption>You can add images with captions!</figcaption>
		</figure>

		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<h2>Overview</h2>
		Give a high-level overview of what you implemented in this homework. Think about what you've built as a whole. Share your thoughts on what interesting things you've learned from completing the homework.

		<h2>Part 1: Ray Generation and Scene Intersection</h2>
		<h3>Ray Generation</h3>
		As stated in the specs, we transform from image to camera coordinates, generate a ray, which is then transformed into world space. To get into camera space, we simply shift the image so it's in the z = -1 plane, center it on the z-axis (subtract 0.5 from x, y), then scale it (multiplication by 2 * tan(Fov / 2)). We initialize our ray, and prepare to transform into world space. Ray origin is just the location of our camera in world space, since it's just (0, 0, 0) in camera space. Then, we need to rotate our ray direction into world space using the c2w rotation matrix. Ray generation is complete.
		<h3>Pixel Sampling</h3>
		Here, we randomly cast a ray from the camera to a spot within each pixel ns_aa times, get the radiance, and accumulate it into our Monte Carlo estimator, Fn. For reference, Fn = 1 / ns_aa * âˆ‘est_radiance_global_illumination(ray) / p(ray), where p(ray) = 1 because we are in a uniform, continuous distribution, and the 2D area we are sampling over is of size 1. Using the estimate, we update the pixel value within our sampleBuffer.
		<h3>Intersecting Primitives</h3>
		For triangles, I use the Moller Trumbore algorithm, whcih solves for a t (if the ray isn't parallel to the plane of the triangle) and barycentric coordinates of intersection, b1, b2, and b1 + b2. If all barycentric coordinates are in [0, 1], then the point of intersection lies within the triangle. 
		<br>
		For spheres, I follow the lecture slides, solving for real roots of a quadratic equation. Then, checking for the closest valid t value (within r.max_t and r.min_t), we return true if a valid intersection is detected.
		<p>Here are some images with normal shading.</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
				<tr>
					<td style="text-align: center;">
						<img src="1-banana.png" width="480px"/>
						<figcaption>Banana</figcaption>
					</td>
					<td style="text-align: center;">
						<img src="1-spheres.png" width="480px"/>
						<figcaption>Spheres</figcaption>
					</td>
				</tr>
			  	<tr>
					<td style="text-align: center;">
				  		<img src="1-bench.png" width="480px"/>
				  		<figcaption>Bench</figcaption>
					</td>
					<td style="text-align: center;">
				  		<img src="1-bunny.png" width="480px"/>
				  		<figcaption>Bunny</figcaption>
					</td>
			  	</tr>
			</table>
		</div>
		
		<h2>Part 2: Bounding Volume Hierarchy</h2>
		<h3>BVH Generation</h3>
		<ol type="1">
  			<li>Construct the bounding box for the node. This is necessary for all nodes, interior or leaf. It is also convenient to find the min and max values for the centroids of our primitives along each axis. This is used later in step 3.</li>
			<li>Check the base case: if the number of primitives is lower than parameter max_leaf_size, assign the start and end iterators and return the node.</li>
			<li>Determine the split:
				<ol type="a">
					<li>Split Axis: For my implementation, I chose the axis with the farthest distance between centroids. Centroid min max coordinates were precomputed in step 1.</li>
      				<li>Split Coordinate: Average centroids coordinates along the axis of split. Note: an optimization would be to also compute this in step 1 to avoid needing another loop over the primitives.</li>
    			</ol>
			</li>
			<li>Sort primitives by the split, and find the split item (the first item in the right child). Simple iteration here, but first,  sort the primitive pointers (given as input by the iterators, start and end) along the axis. This can prevent iterator assignment bugs (like node->start being ahead of node->end). Additionally, we can early break from the sorting loop after getting the split item.</li>
			<li>Make recursive calls and assign children of the current node. The left child is assigned primitives from start to split, and the right child is assigned primitives from split to end.</li>
		</ol>
		Here are some images of large dae files (100k+ primitives) that should not be rendered without BVH.
		<figure>
			<img src="2-CBlucy.png" alt"Statue of Angel"/>
			<figcaption>CBlucy: 133,796 primitives</figcaption>
		</figure>
		<figure>
			<img src="2-blob.png" alt"Blob"/>
			<figcaption>Blob: 196,608 primitives</figcaption>
		</figure>

		<h3>Performance Comparison</h3>
		All images analyzed below were rendered with 1 thread for a resolution of 800 x 600. <br>
		<table>
  			<tr>
    			<th>Image</th>
    			<th># Primitives</th>
    			<th>Time (unoptimized) (s)</th>
    			<th>Time (BVH) (s)</th>
  			</tr>
  			<tr>
    			<td>banana</td>
    			<td>2,458</td>
    			<td>14.0737</td>
    			<td>0.1637</td>
  			</tr>
  			<tr>
    			<td>cow</td>
    			<td>5,856</td>
    			<td>40.7381</td>
    			<td>0.2415</td>
  			</tr>
  			<tr>
    			<td>bunny</td>
    			<td>33,696</td>
    			<td>322.0565</td>
    			<td>0.2552</td>
  			</tr>
  			<tr>
    			<td>max_planck</td>
    			<td>50,801</td>
    			<td>-</td>
    			<td>0.3361</td>
  			</tr>
  			<tr>
    			<td>bench</td>
    			<td>67,808</td>
    			<td>1,386.7399</td>
    			<td>0.1619</td>
  			</tr>
  			<tr>
    			<td>CBlucy</td>
    			<td>133,796</td>
    			<td>-</td>
    			<td>0.2514</td>
  			</tr>
  			<tr>
    			<td>blob</td>
    			<td>196,608</td>
    			<td>-</td>
    			<td>0.4357</td>
  			</tr>
		</table>
		As can be seen above, BVH renders all images under 1 second. This is incredible, especially since it's not linearly scaling to the increase in the number of primitives. An image with 2,458 primitives took 0.1637 seconds to render, while an image with 80x the number of primitives a little less than 3x the time to render. Evidently, BVH is a crucial optimization for ray tracing. Something of note is that even though the bench image has a high number of primitives (67,808), it took a little less time than the banana (2,458). This is likely because most of bench's bounding boxes are hugging the actual bench object itself. As a result, many rays can early return from recursive calls during ray tracing. In theory, this could mean even better optimization with a better split scheme, such as one where primitives for an object tend to be grouped together in the same bounding box. For scenes with many different objects, this would be largely beneficial, and is probably utilized in game engines like Unity that have boxColliders for gameObjects.
		
			
		<h2>Part 3: Direct Illumination</h2>
		<h3>Direct Lighting Functions</h3>
		<ol type="1">
			<li>Iterate for each sample. In uniform sampling, this is straightforwardly just num_samples, while in importance sampling, we must iterate over each light, then each sample for each light. During importance sampling we must also change the num_samples per light based on if it's a point or area light, since point lights will generate the same samples each time (and are considered already averaged).
</li>
			<li>Get the incoming radiance ray w_in. Uniform sampling uses the provided hemisphereSampler.get_sample() to generate a w_in in object space. Importance sampling is assigned a w_in (as well as other useful values L_in, distToLight, pdf) through light->sample_L(...). Note: w_in starts at hit_p and points to the light source (or the next intersection).</li>
			<li>Check if w_in is a shadow ray. Set min_t to EPS_F to mitigate floating point precision errors, then intersect the ray with the scene. For uniform sampling, if there is no hit, or the intersected primitive is not a light source (isect_in.bsdf->get_emission().norm() < EPS_F), end the iteration of the loop here to save computation of a discarded sample. For importance sampling, we can additionally set max_t to distToLight - EPS_F so it won't intersect our light. Opposite of uniform sampling, a hit implies occlusion, so we discard the sample on hit instead. </li>
			<li>Calculate inputs for L_out. costheta can be found with w_in * isect.n if w_in is in world space, or simply w_in.z in object space. bsdf is evaluated by isect.bsdf->f(w_out, w_in) with w_in in object space. L_in is the emission at the intersection from w_in, or alternatively given by light->sample_L(...) mentioned in step 2. pdf is also given by light->sample_L(...), but for uniform sampling, is just 1/(2 * PI) each time. Lastly, N is the num_samples, total in the case of uniform sampling, and for the light in the case of importance sampling. </li>
			<li>Compute the reflectance, accumulate in L_out, and return L_out. L_out := bsdf * L_in * costheta / pdf / num_samples. </li>
		</ol>
		
		<p>Here are some images rendered with Uniform and Importance sampling for comparison.</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
				<tr>
					<td style="text-align: center;">
						<img src="3-CBspheres_lambertian_uniform.png" width="480px"/>
						<figcaption>Spheres: Uniform</figcaption>
					</td>
					<td style="text-align: center;">
						<img src="3-CBspheres_lambertian_importance.png" width="480px"/>
						<figcaption>Spheres: Importance</figcaption>
					</td>
				</tr>
			  	<tr>
					<td style="text-align: center;">
				  		<img src="3-CBbunny_uniform.png" width="480px"/>
				  		<figcaption>Bunny: Uniform</figcaption>
					</td>
					<td style="text-align: center;">
				  		<img src="3-CBbunny_importance.png" width="480px"/>
				  		<figcaption>Bunny: Importance</figcaption>
					</td>
			  	</tr>
			</table>
		</div>

		<p>Here is a dragon rendered with 1 sample per pixel with an increasing number of light rays.</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
				<tr>
					<td style="text-align: center;">
						<img src="3-dragon_1L.png" width="480px"/>
						<figcaption>1 Light Ray</figcaption>
					</td>
					<td style="text-align: center;">
						<img src="3-dragon_4L.png" width="480px"/>
						<figcaption>4 Light Ray</figcaption>
					</td>
				</tr>
			  	<tr>
					<td style="text-align: center;">
				  		<img src="3-dragon_16L.png" width="480px"/>
				  		<figcaption>16 Light Ray</figcaption>
					</td>
					<td style="text-align: center;">
				  		<img src="3-dragon_64L.png" width="480px"/>
				  		<figcaption>64 Light Ray</figcaption>
					</td>
			  	</tr>
			</table>
		</div>
		<h3>Uniform vs Importance</h3>
		I assumed that Light Sampling should take less time, since it likely generates a lower percentage of useless samples that don't even reach a light source. However, in practice they take similar amounts of time. This is probably because intersection is the bottleneck cost. We are performing the same amounts of intersection with either algorithm, which is why neither of them hold an advantage time-wise. In terms of performance, Light Sampling is clearly the better option. Uniform Sampling produces images that remain grainy even as we increase the number of samples per pixel or the amount of light rays. However, even with a high amount of light rays, with only 1 sample per pixel, the rendered image still looks a little noisy. There are probably diminishing returns here, with better quality images possible by balancing samples per pixel and the amount of light rays.
		<h2>Part 4: Global Illumination</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

		<h2>Part 5: Adaptive Sampling</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

		<h2>(Optional) Part 6: Extra Credit Opportunities</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
		
		<h2>Additional Notes (please remove)</h2>
		<ul>
			<li>You can also add code if you'd like as so: <code>code code code</code></li>
			<li>If you'd like to add math equations, 
				<ul>
					<li>You can write inline equations like so: \( a^2 + b^2 = c^2 \)</li>
					<li>You can write display equations like so: \[ a^2 + b^2 = c^2 \]</li>
				</ul>
			</li>
		</ul>
		</div>
	</body>
</html>
